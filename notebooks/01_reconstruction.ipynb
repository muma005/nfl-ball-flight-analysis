{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "68a2cbfe",
      "metadata": {
        "id": "68a2cbfe"
      },
      "source": [
        "# 01 - Reconstruction\n",
        "\n",
        "Reconstruct speed, acceleration, direction, and other derived signals from position data."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5PboHlyPOf3"
      },
      "id": "Z5PboHlyPOf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Letâ€™s now build a **step-by-step guided version** â€”\n",
        "think of it as your **â€œMerge & Reconstruction Playbookâ€** ðŸ§ âš™ï¸ â€”\n",
        "with each step having:\n",
        "\n",
        "* ðŸŽ¯ *Purpose (why we do it)*\n",
        "* ðŸ§© *Action (what exactly to code/do)*\n",
        "* ðŸ§  *Validation (how to confirm it worked)*\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ§­ MERGE & RECONSTRUCTION PLAYBOOK\n",
        "\n",
        "**Goal:** Safely merge `input`, `output`, and `supplementary` tracking data\n",
        "into one continuous, consistent dataset ready for metric computation.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 1 â€” Load and Inspect\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "Get familiar with what youâ€™re merging â€” shape, columns, basic counts.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "input_df = pd.read_parquet('data/raw/week1_input.parquet')\n",
        "output_df = pd.read_parquet('data/raw/week1_output.parquet')\n",
        "supp_df = pd.read_csv('data/raw/supplementary_data.csv')\n",
        "\n",
        "print(input_df.shape, output_df.shape, supp_df.shape)\n",
        "print(input_df.columns)\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… Files load without error.\n",
        "âœ… Columns match expectations (`game_id`, `play_id`, `nfl_id`, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 2 â€” Validate Keys & Uniqueness\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "Prevent silent ID mixups that could ruin reconstruction.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "print(\"Input duplicates:\", input_df.duplicated(['game_id','play_id','nfl_id','frame_id']).sum())\n",
        "print(\"Output duplicates:\", output_df.duplicated(['game_id','play_id','nfl_id','frame_id']).sum())\n",
        "print(\"Supplementary duplicates:\", supp_df.duplicated(['game_id','play_id']).sum())\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… All zero (or very low if legitimate).\n",
        "âœ… Any duplicates should be inspected â€” they mean repeated records for same frame/player.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 3 â€” Attribute Extraction\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "We donâ€™t want to lose player metadata (height, role, etc.) during merge.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "player_attrs = input_df[['game_id','play_id','nfl_id','player_name','player_height',\n",
        "                         'player_weight','player_birth_date','player_position','player_side',\n",
        "                         'player_role','num_frames_output','ball_land_x','ball_land_y']].drop_duplicates()\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… Each `(game_id, play_id, nfl_id)` unique.\n",
        "âœ… No missing critical info (like player_name).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 4 â€” Frame Continuity & Normalization\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "Output data restarts frame numbering at 1 â†’ we must realign them so\n",
        "input + output form one continuous timeline.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "# Find max input frame for each player per play\n",
        "max_input_frame = input_df.groupby(['game_id','play_id','nfl_id'])['frame_id'].max().reset_index()\n",
        "max_input_frame.rename(columns={'frame_id':'last_input_frame'}, inplace=True)\n",
        "\n",
        "# Merge into output and shift frame_ids\n",
        "output_df = output_df.merge(max_input_frame, on=['game_id','play_id','nfl_id'], how='left')\n",
        "output_df['frame_id_shifted'] = output_df['frame_id'] + output_df['last_input_frame']\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… Each playerâ€™s timeline continues smoothly (no overlaps).\n",
        "âœ… `frame_id_shifted` increases after input frames.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 5 â€” Merge Attributes Into Frames\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "Ensure both input and output frames carry static player information.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "input_df = input_df.merge(player_attrs, on=['game_id','play_id','nfl_id'], how='left')\n",
        "output_df = output_df.merge(player_attrs, on=['game_id','play_id','nfl_id'], how='left')\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… After merge, both have equal player-level fields.\n",
        "âœ… No unexpected NaNs introduced.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 6 â€” Concatenate Input + Output\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "Build a single, chronological sequence from pre-throw â†’ post-throw.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "output_df = output_df.drop(columns=['frame_id']).rename(columns={'frame_id_shifted':'frame_id'})\n",
        "combined_df = pd.concat([input_df, output_df], ignore_index=True)\n",
        "combined_df.sort_values(['game_id','play_id','nfl_id','frame_id'], inplace=True)\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… Each playerâ€™s frames are sequential and increasing.\n",
        "âœ… Plot one playerâ€™s xâ€“y over time to visually confirm smoothness.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 7 â€” Merge Play Outcomes\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "Attach the play results (e.g., complete, incomplete) from supplementary data.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "reconstructed_df = combined_df.merge(supp_df, on=['game_id','play_id'], how='left', validate='m:1')\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… No mismatched or missing plays.\n",
        "âœ… Number of rows unchanged.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 8 â€” Quality Evaluation\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "Confirm data integrity after full reconstruction.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "print(\"Missing data ratio:\\n\", reconstructed_df.isna().mean())\n",
        "print(\"Unique player count check:\",\n",
        "      len(reconstructed_df['nfl_id'].unique()),\n",
        "      \"vs\", len(input_df['nfl_id'].unique()))\n",
        "\n",
        "# Check continuity per player\n",
        "gap_check = reconstructed_df.groupby(['game_id','play_id','nfl_id'])['frame_id'].diff().dropna()\n",
        "print(\"Frame gaps:\", (gap_check > 1).sum())\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… Missing < 1â€“2% (mostly tolerable metadata).\n",
        "âœ… Frame gaps = 0 (continuous motion).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 9 â€” Visual Verification (Optional)\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "Human-eye validation to confirm continuity of player trajectories.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample = reconstructed_df.query(\"game_id==2023091001 and play_id==4216\")\n",
        "for pid in sample['nfl_id'].unique():\n",
        "    p = sample[sample['nfl_id']==pid]\n",
        "    plt.plot(p['x'], p['y'], label=pid)\n",
        "plt.title(\"Reconstructed Player Motion\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… Smooth player paths â†’ no teleportation (inputâ†’output transition continuous).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Step 10 â€” Save & Document\n",
        "\n",
        "**ðŸŽ¯ Purpose:**\n",
        "Persist clean, validated reconstruction for feature engineering.\n",
        "\n",
        "**ðŸ§© Action:**\n",
        "\n",
        "```python\n",
        "output_path = Path(\"data/processed/reconstructed.parquet\")\n",
        "reconstructed_df.to_parquet(output_path, index=False, compression='snappy')\n",
        "```\n",
        "\n",
        "**ðŸ§  Validation:**\n",
        "âœ… File saved, correct row count.\n",
        "âœ… Add log entry:\n",
        "\n",
        "```python\n",
        "print(f\"âœ… Reconstruction complete â€” {len(reconstructed_df)} rows saved at {output_path}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§¾ End Deliverable\n",
        "\n",
        "âœ… `data/processed/reconstructed.parquet`\n",
        "âœ… Merge validation summary (NaNs, frame continuity, sample plot)\n"
      ],
      "metadata": {
        "id": "vKo4IbzFN_wx"
      },
      "id": "vKo4IbzFN_wx"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PEGPJjlXPLE2"
      },
      "id": "PEGPJjlXPLE2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}